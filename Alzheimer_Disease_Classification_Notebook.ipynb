{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alzheimer_Disease_Classification_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/ADMRILSE/blob/main/Alzheimer_Disease_Classification_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeEJa2_TZnec"
      },
      "source": [
        "## **AD-MRI-LSE: Alzheimer Disease Classification From Structural MR Images Using Deep Latent Space Encoding**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-m_byicZkQc"
      },
      "source": [
        "This code provide Tensorflor-Keras implementation of AD-MRI-LSE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LadaflfshqgF"
      },
      "source": [
        "# Loading Useful packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si6012nzZktl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77796edc-a7be-4453-abcf-3f7be23ea409"
      },
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "from random import sample\n",
        "\n",
        "import keras\n",
        "# import os.path\n",
        "from os import path, listdir\n",
        "import h5py\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wget\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.io import loadmat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=bdf2e4602a82b5ea2f30c306d785b9ccb8247d45a141e2d04b3b0e5b91e45137\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAMR_JN60EoK",
        "outputId": "750946ef-cf3b-4214-f744-4c4b05aa4b0d"
      },
      "source": [
        "!python --version\n",
        "print('Tensorflow:',tf.__version__)\n",
        "print('Keras:',keras.__version__)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n",
            "Tensorflow: 2.4.1\n",
            "Keras: 2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgIp3d0U-RHC"
      },
      "source": [
        "# Downloading dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB-LjEPU97rw"
      },
      "source": [
        "**Loading and processing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK4g9LZ0L9Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f00bedc-80b3-4e1c-aabe-9891c94d53b8"
      },
      "source": [
        "classes = ('AD_F','AD_M','NC_F','NC_M')\n",
        "# num_AD_F = 137\n",
        "# num_AD_M = 157\n",
        "# num_NC_F = 186\n",
        "# num_NC_M = 187\n",
        "\n",
        "num_AD_F = 10 #137\n",
        "num_AD_M = num_AD_F\n",
        "num_NC_F = num_AD_F\n",
        "num_NC_M = num_AD_F\n",
        "\n",
        "num_samples = num_AD_F+num_AD_M+num_NC_F+num_NC_M\n",
        "num_train = np.round(0.8*num_samples/4)\n",
        "num_val = np.round(0.1*num_samples/4)\n",
        "\n",
        "\n",
        "AD_F_labels = np.zeros((num_AD_F,1))\n",
        "AD_M_labels = np.zeros((num_AD_M,1))\n",
        "NC_F_labels = np.ones((num_NC_F,1))\n",
        "NC_M_labels = np.ones((num_NC_M,1))\n",
        "\n",
        "Class_labels = np.concatenate((AD_F_labels,AD_M_labels,NC_F_labels,NC_M_labels), axis = 0)\n",
        "\n",
        "data_path = 'https://raw.githubusercontent.com/Shujaat123/ADMRILSE/master/ADNI_mni_normalized_rescaled_DB/'\n",
        "\n",
        "AD_F_scans = []\n",
        "AD_M_scans = []\n",
        "NC_F_scans = []\n",
        "NC_M_scans = []\n",
        "\n",
        "for Cname in classes:\n",
        "  if(Cname=='AD_F'):\n",
        "    num_sub = num_AD_F\n",
        "    flist = []\n",
        "  elif(Cname=='AD_M'):\n",
        "    num_sub = num_AD_M\n",
        "    flist = []\n",
        "  elif(Cname=='NC_F'):\n",
        "    num_sub = num_NC_F\n",
        "    flist = []\n",
        "  else:\n",
        "    num_sub = num_NC_M\n",
        "    flist = []\n",
        "  \n",
        "  for sub_loop in range(0,num_sub):\n",
        "    # print(\"{:0>3d}\".format(sub_loop+1))\n",
        "    filename = Cname+'_subject_'+'{:0>3d}'.format(sub_loop+1)+'.mat'\n",
        "    # print(filename)\n",
        "    # if(path.exists(filename)):\n",
        "    #   !rm $filename\n",
        "    #   print('existing file:', filename, ' has been deleted')\n",
        "    # print('downloading latest version of file:', filename)\n",
        "    # file_path = data_path + Cname + '/' + filename\n",
        "    # wget.download(file_path, filename)\n",
        "    if(not(path.exists(filename))):\n",
        "      print('downloading latest version of file:', filename)\n",
        "      file_path = data_path + Cname + '/' + filename\n",
        "      wget.download(file_path, filename)\n",
        "\n",
        "    print('Loading data file:', filename)  \n",
        "    MR_img = loadmat(filename)['re_mri_scan']\n",
        "    flist.append(MR_img)\n",
        "  \n",
        "  if(Cname=='AD_F'):\n",
        "    AD_F_scans=np.asarray(flist)\n",
        "  elif(Cname=='AD_M'):\n",
        "    AD_M_scans=np.asarray(flist)\n",
        "  elif(Cname=='NC_F'):\n",
        "    NC_F_scans=np.asarray(flist)\n",
        "  else:\n",
        "    NC_M_scans=np.asarray(flist)\n",
        "print('DONE')\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading latest version of file: AD_F_subject_001.mat\n",
            "Loading data file: AD_F_subject_001.mat\n",
            "downloading latest version of file: AD_F_subject_002.mat\n",
            "Loading data file: AD_F_subject_002.mat\n",
            "downloading latest version of file: AD_F_subject_003.mat\n",
            "Loading data file: AD_F_subject_003.mat\n",
            "downloading latest version of file: AD_F_subject_004.mat\n",
            "Loading data file: AD_F_subject_004.mat\n",
            "downloading latest version of file: AD_F_subject_005.mat\n",
            "Loading data file: AD_F_subject_005.mat\n",
            "downloading latest version of file: AD_F_subject_006.mat\n",
            "Loading data file: AD_F_subject_006.mat\n",
            "downloading latest version of file: AD_F_subject_007.mat\n",
            "Loading data file: AD_F_subject_007.mat\n",
            "downloading latest version of file: AD_F_subject_008.mat\n",
            "Loading data file: AD_F_subject_008.mat\n",
            "downloading latest version of file: AD_F_subject_009.mat\n",
            "Loading data file: AD_F_subject_009.mat\n",
            "downloading latest version of file: AD_F_subject_010.mat\n",
            "Loading data file: AD_F_subject_010.mat\n",
            "downloading latest version of file: AD_M_subject_001.mat\n",
            "Loading data file: AD_M_subject_001.mat\n",
            "downloading latest version of file: AD_M_subject_002.mat\n",
            "Loading data file: AD_M_subject_002.mat\n",
            "downloading latest version of file: AD_M_subject_003.mat\n",
            "Loading data file: AD_M_subject_003.mat\n",
            "downloading latest version of file: AD_M_subject_004.mat\n",
            "Loading data file: AD_M_subject_004.mat\n",
            "downloading latest version of file: AD_M_subject_005.mat\n",
            "Loading data file: AD_M_subject_005.mat\n",
            "downloading latest version of file: AD_M_subject_006.mat\n",
            "Loading data file: AD_M_subject_006.mat\n",
            "downloading latest version of file: AD_M_subject_007.mat\n",
            "Loading data file: AD_M_subject_007.mat\n",
            "downloading latest version of file: AD_M_subject_008.mat\n",
            "Loading data file: AD_M_subject_008.mat\n",
            "downloading latest version of file: AD_M_subject_009.mat\n",
            "Loading data file: AD_M_subject_009.mat\n",
            "downloading latest version of file: AD_M_subject_010.mat\n",
            "Loading data file: AD_M_subject_010.mat\n",
            "downloading latest version of file: NC_F_subject_001.mat\n",
            "Loading data file: NC_F_subject_001.mat\n",
            "downloading latest version of file: NC_F_subject_002.mat\n",
            "Loading data file: NC_F_subject_002.mat\n",
            "downloading latest version of file: NC_F_subject_003.mat\n",
            "Loading data file: NC_F_subject_003.mat\n",
            "downloading latest version of file: NC_F_subject_004.mat\n",
            "Loading data file: NC_F_subject_004.mat\n",
            "downloading latest version of file: NC_F_subject_005.mat\n",
            "Loading data file: NC_F_subject_005.mat\n",
            "downloading latest version of file: NC_F_subject_006.mat\n",
            "Loading data file: NC_F_subject_006.mat\n",
            "downloading latest version of file: NC_F_subject_007.mat\n",
            "Loading data file: NC_F_subject_007.mat\n",
            "downloading latest version of file: NC_F_subject_008.mat\n",
            "Loading data file: NC_F_subject_008.mat\n",
            "downloading latest version of file: NC_F_subject_009.mat\n",
            "Loading data file: NC_F_subject_009.mat\n",
            "downloading latest version of file: NC_F_subject_010.mat\n",
            "Loading data file: NC_F_subject_010.mat\n",
            "downloading latest version of file: NC_M_subject_001.mat\n",
            "Loading data file: NC_M_subject_001.mat\n",
            "downloading latest version of file: NC_M_subject_002.mat\n",
            "Loading data file: NC_M_subject_002.mat\n",
            "downloading latest version of file: NC_M_subject_003.mat\n",
            "Loading data file: NC_M_subject_003.mat\n",
            "downloading latest version of file: NC_M_subject_004.mat\n",
            "Loading data file: NC_M_subject_004.mat\n",
            "downloading latest version of file: NC_M_subject_005.mat\n",
            "Loading data file: NC_M_subject_005.mat\n",
            "downloading latest version of file: NC_M_subject_006.mat\n",
            "Loading data file: NC_M_subject_006.mat\n",
            "downloading latest version of file: NC_M_subject_007.mat\n",
            "Loading data file: NC_M_subject_007.mat\n",
            "downloading latest version of file: NC_M_subject_008.mat\n",
            "Loading data file: NC_M_subject_008.mat\n",
            "downloading latest version of file: NC_M_subject_009.mat\n",
            "Loading data file: NC_M_subject_009.mat\n",
            "downloading latest version of file: NC_M_subject_010.mat\n",
            "Loading data file: NC_M_subject_010.mat\n",
            "DONE\n",
            "AD_F_subject_001.mat  AD_M_subject_005.mat  NC_F_subject_009.mat\n",
            "AD_F_subject_002.mat  AD_M_subject_006.mat  NC_F_subject_010.mat\n",
            "AD_F_subject_003.mat  AD_M_subject_007.mat  NC_M_subject_001.mat\n",
            "AD_F_subject_004.mat  AD_M_subject_008.mat  NC_M_subject_002.mat\n",
            "AD_F_subject_005.mat  AD_M_subject_009.mat  NC_M_subject_003.mat\n",
            "AD_F_subject_006.mat  AD_M_subject_010.mat  NC_M_subject_004.mat\n",
            "AD_F_subject_007.mat  NC_F_subject_001.mat  NC_M_subject_005.mat\n",
            "AD_F_subject_008.mat  NC_F_subject_002.mat  NC_M_subject_006.mat\n",
            "AD_F_subject_009.mat  NC_F_subject_003.mat  NC_M_subject_007.mat\n",
            "AD_F_subject_010.mat  NC_F_subject_004.mat  NC_M_subject_008.mat\n",
            "AD_M_subject_001.mat  NC_F_subject_005.mat  NC_M_subject_009.mat\n",
            "AD_M_subject_002.mat  NC_F_subject_006.mat  NC_M_subject_010.mat\n",
            "AD_M_subject_003.mat  NC_F_subject_007.mat  sample_data\n",
            "AD_M_subject_004.mat  NC_F_subject_008.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SmbBott1Ln_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99186b92-2d63-4b8a-dfb2-31c68658f176"
      },
      "source": [
        "AD_F_list = list(range(0,num_AD_F))\n",
        "AD_M_list = list(range(0,num_AD_M))\n",
        "NC_F_list = list(range(0,num_NC_F))\n",
        "NC_M_list = list(range(0,num_NC_M))\n",
        "total_list = AD_F_list + AD_M_list + NC_F_list + NC_M_list\n",
        "\n",
        "print('Number of \\'AD_F\\' samples:',len(AD_F_list))\n",
        "print('Number of \\'AD_M\\' samples:',len(AD_M_list))\n",
        "print('Number of \\'NC_F\\' samples:',len(NC_F_list))\n",
        "print('Number of \\'NC_M\\' samples:',len(NC_M_list))\n",
        "print('Total number of samples:',len(total_list))\n",
        "\n",
        "print(AD_F_scans.shape)\n",
        "print(AD_M_scans.shape)\n",
        "print(NC_F_scans.shape)\n",
        "print(NC_M_scans.shape)\n",
        "\n",
        "print(AD_F_labels.shape)\n",
        "print(AD_M_labels.shape)\n",
        "print(NC_F_labels.shape)\n",
        "print(NC_M_labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of 'AD_F' samples: 10\n",
            "Number of 'AD_M' samples: 10\n",
            "Number of 'NC_F' samples: 10\n",
            "Number of 'NC_M' samples: 10\n",
            "Total number of samples: 40\n",
            "(10, 120, 160, 120)\n",
            "(10, 120, 160, 120)\n",
            "(10, 120, 160, 120)\n",
            "(10, 120, 160, 120)\n",
            "(10, 1)\n",
            "(10, 1)\n",
            "(10, 1)\n",
            "(10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx2y2kYA-IXu"
      },
      "source": [
        "**Generate Training, Validation and Test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNIbsrCL2qZl"
      },
      "source": [
        "## train select XX AD_F, XX AD_M, XX NC_F, and XX NC_M samples\n",
        "num_train = np.round(0.8*num_samples/4).astype(int)\n",
        "num_val = np.round(0.1*num_samples/4).astype(int)\n",
        "\n",
        "AD_F_train = sample(AD_F_list, num_train)\n",
        "AD_M_train = sample(AD_M_list, num_train)\n",
        "NC_F_train = sample(NC_F_list, num_train)\n",
        "NC_M_train = sample(NC_M_list, num_train)\n",
        "\n",
        "\n",
        "train_list = AD_F_train + AD_M_train + NC_F_train + NC_M_train\n",
        "\n",
        "Input_train = np.concatenate((AD_F_scans[AD_F_train],AD_M_scans[AD_M_train],NC_F_scans[NC_F_train],NC_M_scans[NC_M_train]), axis = 0)\n",
        "Label_train = to_categorical(np.concatenate((AD_F_labels[AD_F_train],AD_M_labels[AD_M_train],NC_F_labels[NC_F_train],NC_M_labels[NC_M_train]), axis = 0))\n",
        "\n",
        "# valid select XX AD_F, XX AD_M, XX NC_F, and XX NC_M samples\n",
        "AD_F_val = sample(set(AD_F_list) - set(AD_F_train), num_val)\n",
        "AD_M_val = sample(set(AD_M_list) - set(AD_M_train), num_val)\n",
        "NC_F_val = sample(set(NC_F_list) - set(NC_F_train), num_val)\n",
        "NC_M_val = sample(set(NC_M_list) - set(NC_M_train), num_val)\n",
        "\n",
        "val_list = AD_F_val + AD_M_val + NC_F_val + NC_M_val\n",
        "\n",
        "Input_val = np.concatenate((AD_F_scans[AD_F_val],AD_M_scans[AD_M_val],NC_F_scans[NC_F_val],NC_M_scans[NC_M_val]), axis = 0)\n",
        "Label_val = to_categorical(np.concatenate((AD_F_labels[AD_F_val],AD_M_labels[AD_M_val],NC_F_labels[NC_F_val],NC_M_labels[NC_M_val]), axis = 0))\n",
        "\n",
        "## test\n",
        "AD_F_test = list(set(AD_F_list) - set(AD_F_train) - set(AD_F_val))\n",
        "AD_M_test = list(set(AD_M_list) - set(AD_M_train) - set(AD_M_val))\n",
        "NC_F_test = list(set(NC_F_list) - set(NC_F_train) - set(NC_F_val))\n",
        "NC_M_test = list(set(NC_M_list) - set(NC_M_train) - set(NC_M_val))\n",
        "\n",
        "test_list = list(set(total_list) - set(train_list) - set(val_list))\n",
        "\n",
        "# test_list\n",
        "Input_test = np.concatenate((AD_F_scans[AD_F_test],AD_M_scans[AD_M_test],NC_F_scans[NC_F_test],NC_M_scans[NC_M_test]), axis = 0)\n",
        "Label_test = to_categorical(np.concatenate((AD_F_labels[AD_F_test],AD_M_labels[AD_M_test],NC_F_labels[NC_F_test],NC_M_labels[NC_M_test]), axis = 0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EycaHLfyzt9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659e6c4a-765c-487f-c03e-b2a2225f4f5b"
      },
      "source": [
        "print(70*'-','\\nTraining Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'AD_F\\' samples:',len(AD_F_train))\n",
        "print('Number of \\'AD_M\\' samples:',len(AD_M_train))\n",
        "print('Number of \\'NC_F\\' samples:',len(NC_F_train))\n",
        "print('Number of \\'NC_M\\' samples:',len(NC_M_train))\n",
        "print('Total number of \\'AD\\' samples:',len(AD_F_train)+len(AD_M_train))\n",
        "print('Total number of \\'NC\\' samples:',len(NC_F_train)+len(NC_M_train))\n",
        "print('Total number of samples:',len(train_list))\n",
        "\n",
        "print(70*'-','\\nValidation Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'AD_F\\' samples:',len(AD_F_val))\n",
        "print('Number of \\'AD_M\\' samples:',len(AD_M_val))\n",
        "print('Number of \\'NC_F\\' samples:',len(NC_F_val))\n",
        "print('Number of \\'NC_M\\' samples:',len(NC_M_val))\n",
        "print('Total number of \\'AD\\' samples:',len(AD_F_val)+len(AD_M_val))\n",
        "print('Total number of \\'NC\\' samples:',len(NC_F_val)+len(NC_M_val))\n",
        "print('Total number of samples:',len(val_list))\n",
        "\n",
        "print(70*'-','\\nTest Dataset distribution\\n',70*'-')\n",
        "print('Number of \\'AD_F\\' samples:',len(AD_F_list)-len(AD_F_train)-len(AD_F_val))\n",
        "print('Number of \\'AD_M\\' samples:',len(AD_M_list)-len(AD_M_train)-len(AD_M_val))\n",
        "print('Number of \\'NC_F\\' samples:',len(NC_F_list)-len(NC_F_train)-len(NC_F_val))\n",
        "print('Number of \\'NC_M\\' samples:',len(NC_M_list)-len(NC_M_train)-len(NC_M_val))\n",
        "print('Total number of \\'AD\\' samples:',len(AD_F_test)+len(AD_M_test))\n",
        "print('Total number of \\'NC\\' samples:',len(NC_F_test)+len(NC_M_test))\n",
        "print('Total number of samples:',len(test_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------- \n",
            "Training Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'AD_F' samples: 8\n",
            "Number of 'AD_M' samples: 8\n",
            "Number of 'NC_F' samples: 8\n",
            "Number of 'NC_M' samples: 8\n",
            "Total number of 'AD' samples: 16\n",
            "Total number of 'NC' samples: 16\n",
            "Total number of samples: 32\n",
            "---------------------------------------------------------------------- \n",
            "Validation Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'AD_F' samples: 1\n",
            "Number of 'AD_M' samples: 1\n",
            "Number of 'NC_F' samples: 1\n",
            "Number of 'NC_M' samples: 1\n",
            "Total number of 'AD' samples: 2\n",
            "Total number of 'NC' samples: 2\n",
            "Total number of samples: 4\n",
            "---------------------------------------------------------------------- \n",
            "Test Dataset distribution\n",
            " ----------------------------------------------------------------------\n",
            "Number of 'AD_F' samples: 1\n",
            "Number of 'AD_M' samples: 1\n",
            "Number of 'NC_F' samples: 1\n",
            "Number of 'NC_M' samples: 1\n",
            "Total number of 'AD' samples: 2\n",
            "Total number of 'NC' samples: 2\n",
            "Total number of samples: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bx5va5XqCD"
      },
      "source": [
        "**Define loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YphC6Dpf_aO8"
      },
      "source": [
        "def loss_DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    # tv = tf.image.total_variation(y_true - y_pred)\n",
        "    mae = K.mean(tf.keras.losses.mean_absolute_error(y_true, y_pred))\n",
        "    return K.mean((1.0 - ssim + mae))\n",
        "\n",
        "def DSSIM(y_true, y_pred):\n",
        "    ssim =tf.image.ssim(y_true, y_pred, 1.0)\n",
        "    return K.mean(ssim)\n",
        "\n",
        "def DPSNR(y_true, y_pred):\n",
        "    psnr =tf.image.psnr(y_true, y_pred,1)\n",
        "    return K.mean(psnr)\n",
        "\n",
        "def dsc(y_true, y_pred):\n",
        "    smooth_ = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth_) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth_)\n",
        "    return score\n",
        "\n",
        "# def dice_loss(y_true, y_pred):\n",
        "    # loss = 1 - dsc(y_true, y_pred)\n",
        "    # return loss\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
        "\n",
        "  return 1 - (numerator + 1) / (denominator + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbacT_QSs2Yu"
      },
      "source": [
        "**Define performance measures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufIMP-uLzFaT"
      },
      "source": [
        "def pmeasure(y_true, y_pred):\n",
        "  cm = confusion_matrix(y_true.ravel(), y_pred.ravel())\n",
        "  tp_0 = cm[0][0]\n",
        "  tn_0 = cm[1][1]\n",
        "  fp_0 = cm[0][1]\n",
        "  fn_0 = cm[1][0]\n",
        "\n",
        "  sensitivity_0 = tp_0 / (tp_0 + fn_0)\n",
        "  specificity_0 = tn_0 / (tn_0 + fp_0)\n",
        "\n",
        "  f1_score_0 = 2 * tp_0 / (2 * (tp_0 + fp_0 + fn_0))\n",
        " \n",
        "\n",
        "  return ({'Sensitivity': sensitivity_0,\n",
        "           'Specificity': specificity_0,\n",
        "           'F1-Score': f1_score_0})\n",
        "  \n",
        "def check_preds(ypred, ytrue):\n",
        "    smooth = 1\n",
        "    pred = np.ndarray.flatten(np.clip(ypred, 0, 1))\n",
        "    gt = np.ndarray.flatten(np.clip(ytrue, 0, 1))\n",
        "    intersection = np.sum(pred * gt) \n",
        "    union = np.sum(pred) + np.sum(gt)   \n",
        "    return np.round((2 * intersection + smooth)/(union + smooth), decimals=5)\n",
        "\n",
        "def auc(y_true, y_pred):\n",
        "    smooth = 1\n",
        "    y_pred_pos = np.round(np.clip(y_pred, 0, 1))\n",
        "    y_pred_neg = 1 - y_pred_pos\n",
        "    y_pos = np.round(np.clip(y_true, 0, 1))\n",
        "    y_neg = 1 - y_pos\n",
        "    tp = np.sum(y_pos * y_pred_pos)\n",
        "    tn = np.sum(y_neg * y_pred_neg)\n",
        "    fp = np.sum(y_neg * y_pred_pos)\n",
        "    fn = np.sum(y_pos * y_pred_neg)\n",
        "    tpr = (tp + smooth) / (tp + fn + smooth)  # recall\n",
        "    tnr = (tn + smooth) / (tn + fp + smooth)\n",
        "    prec = (tp + smooth) / (tp + fp + smooth)  # precision\n",
        "    return [tpr, tnr, prec]\n",
        "  \n",
        "def dice_score(y_true, y_pred, thres):\n",
        "  dice = np.zeros(y_true.shape[0])\n",
        "  dsc_thres = np.zeros_like(dice)\n",
        "  recall = np.zeros_like(dice)\n",
        "  precision = np.zeros_like(dice)\n",
        "\n",
        "  for i in range(y_true.shape[0]):\n",
        "    ds = dsc(y_true[i], y_pred[i])\n",
        "    dice[i] = K.eval(ds)\n",
        "    dsc_thres[i] = check_preds(y_pred[i] > thres, y_true[i])\n",
        "    recall[i], _, precision[i] = auc(y_true[i], y_pred[i] > thres)\n",
        "  \n",
        "  dice = np.mean(dice)\n",
        "  dsc_thres = np.mean(dsc_thres)\n",
        "  recall = np.mean(recall)\n",
        "  precision = np.mean(precision)\n",
        "\n",
        "  return {'Dice score': dice, 'Dice score with threshold': dsc_thres, 'Recall': recall, 'Precision': precision}\n",
        "\n",
        "def Show_Statistics(msg, Stat):\n",
        "    print(msg.upper())\n",
        "    print(70 * '-')\n",
        "    print('Accuracy:', Stat[0])\n",
        "\n",
        "    print('Sensitivity:', Stat[1])\n",
        "    print('Specificity:', Stat[2])\n",
        "    print('F1-Score:', Stat[3])\n",
        "\n",
        "    print('Balance Accuracy:', Stat[4])\n",
        "    print(70 * '-')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9mtHsEaAyA"
      },
      "source": [
        "# Designing an Auto-Encoder-based classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge7uF0idR-Gc"
      },
      "source": [
        "def BUS_Final_Model(num_filters=8,input_shape=(120, 160, 120, 1)):\n",
        "    # Encoder Network\n",
        "    encoder_input = Input(shape=input_shape, name='encoder_input')\n",
        "    enc_l1 = Conv3D(num_filters, 15, activation='relu', name='encoder_layer1', padding='same')(encoder_input)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Conv3D(num_filters, 3, activation='relu', name='encoder_layer2', padding='same')(enc_l1)\n",
        "    enc_l1 = BatchNormalization()(enc_l1)\n",
        "    enc_l1 = Dropout(0.4)(enc_l1)\n",
        "    enc_l1 = MaxPooling3D(pool_size=(8, 8, 8))(enc_l1)\n",
        "    # enc_l1 = MaxPooling2D(pool_size=(4, 4))(enc_l1)\n",
        "\n",
        "    # enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer3', padding='same')(enc_l1)\n",
        "    # enc_l2 = BatchNormalization()(enc_l2)\n",
        "    # enc_l2 = Conv2D(2 * num_filters, 3, activation='relu', name='encoder_layer4', padding='same')(enc_l2)\n",
        "    # enc_l2 = BatchNormalization()(enc_l2)\n",
        "    # # enc_l2 = Dropout(0.4)(enc_l2)\n",
        "    # enc_l2 = MaxPooling2D(pool_size=(2, 2))(enc_l2)\n",
        "\n",
        "    # enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer5', padding='same')(enc_l2)\n",
        "    # enc_l3 = BatchNormalization()(enc_l3)\n",
        "    # enc_l3 = Conv2D(4 * num_filters, 3, activation='relu', name='encoder_layer6', padding='same')(enc_l3)\n",
        "    # enc_l3 = BatchNormalization()(enc_l3)\n",
        "    # # enc_l3 = Dropout(0.4)(enc_l3)\n",
        "    # enc_l3 = MaxPooling2D(pool_size=(2, 2))(enc_l3)\n",
        "\n",
        "    # enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer7', padding='same')(enc_l3)\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\n",
        "    # enc_l4 = Conv2D(8 * num_filters, 3, activation='relu', name='encoder_layer8', padding='same')(enc_l4)\n",
        "    # enc_l4 = BatchNormalization()(enc_l4)\n",
        "    # # enc_l4 = Dropout(0.4)(enc_l4)\n",
        "    # enc_l4 = MaxPooling2D(pool_size=(2, 2))(enc_l4)\n",
        "\n",
        "    # enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer9', padding='same')(enc_l4)\n",
        "    # enc_l5 = BatchNormalization()(enc_l5)\n",
        "    # enc_l5 = Conv2D(16 * num_filters, 3, activation='relu', name='encoder_layer10', padding='same')(enc_l5)\n",
        "    # enc_l5 = BatchNormalization()(enc_l5)\n",
        "    # enc_l5 = Dropout(0.4)(enc_l5)\n",
        "    # enc_l5 = MaxPooling2D(pool_size=(2, 2))(enc_l5)\n",
        "\n",
        "    # encoder_output = Conv2D(32 * num_filters, 3, activation='relu', name='encoder_output', padding='same')(enc_l5)\n",
        "    encoder_output = Conv3D(num_filters, 3, activation='relu', name='encoder_output', padding='same')(enc_l1)\n",
        "\n",
        "    # Classifier Network\n",
        "    flat = Flatten()(encoder_output)\n",
        "    class_l1 = Dense(8, activation='relu', name='class_layer1')(flat)\n",
        "    # class_l1 = BatchNormalization()(class_l1)\n",
        "    # drop_c1 = Dropout(0.4)(class_l1)\n",
        "    # class_l2 = Dense(512, activation='relu', name='class_layer2')(drop_c1)\n",
        "    # class_l2 = BatchNormalization()(class_l2)\n",
        "    # drop_c2 = Dropout(0.4)(class_l2)\n",
        "    # class_l3 = Dense(256, activation='relu', name='class_layer3')(drop_c2)\n",
        "    # class_l3 = BatchNormalization()(class_l3)\n",
        "    # drop_c3 = Dropout(0.4)(class_l3)\n",
        "    # class_output = Dense(4, activation='softmax', name='class_output')(drop_c3)\n",
        "    class_output = Dense(2, activation='softmax', name='class_output')(class_l1)\n",
        "\n",
        "    # # Decoder Network\n",
        "    # # dec_l1 = UpSampling2D(size=(2, 2))(concatenate([enc_l5, encoder_output], axis=3))\n",
        "    # dec_l1 = UpSampling2D(size=(2, 2))(encoder_output)\n",
        "    # dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer1', padding='same')(dec_l1)\n",
        "    # dec_l1 = BatchNormalization()(dec_l1)\n",
        "    # dec_l1 = Conv2D(16 * num_filters, 3, activation='relu', name='decoder_layer2', padding='same')(dec_l1)\n",
        "    # dec_l1 = BatchNormalization()(dec_l1)\n",
        "    # # dec_l1 = Dropout(0.4)(dec_l1)\n",
        "\n",
        "    # # dec_l2 = UpSampling2D(size=(2, 2))(concatenate([enc_l4, dec_l1], axis=3))\n",
        "    # dec_l2 = UpSampling2D(size=(2, 2))(dec_l1)\n",
        "    # dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer3', padding='same')(dec_l2)\n",
        "    # # dec_l2 = BatchNormalization()(dec_l2)\n",
        "    # dec_l2 = Conv2D(8 * num_filters, 3, activation='relu', name='decoder_layer4', padding='same')(dec_l2)\n",
        "    # dec_l2 = BatchNormalization()(dec_l2)\n",
        "    # # dec_l2 = Dropout(0.4)(dec_l2)\n",
        "\n",
        "    # # dec_l3 = UpSampling2D(size=(2, 2))(concatenate([enc_l3, dec_l2], axis=3))\n",
        "    # dec_l3 = UpSampling2D(size=(2, 2))(dec_l2)\n",
        "    # dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer5', padding='same')(dec_l3)\n",
        "    # # dec_l3 = BatchNormalization()(dec_l3)\n",
        "    # dec_l3 = Conv2D(2 * num_filters, 3, activation='relu', name='decoder_layer6', padding='same')(dec_l3)\n",
        "    # dec_l3 = BatchNormalization()(dec_l3)\n",
        "    # # dec_l3 = Dropout(0.4)(dec_l3)\n",
        "\n",
        "    # # dec_l4 = UpSampling2D(size=(2, 2))(concatenate([enc_l2, dec_l3], axis=3))\n",
        "    # dec_l4 = UpSampling2D(size=(2, 2))(dec_l3)\n",
        "    # dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer7', padding='same')(dec_l4)\n",
        "    # # dec_l4 = BatchNormalization()(dec_l4)\n",
        "    # dec_l4 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer8', padding='same')(dec_l4)\n",
        "    # dec_l4 = BatchNormalization()(dec_l4)\n",
        "    # # dec_l4 = Dropout(0.4)(dec_l4)\n",
        "\n",
        "    # # dec_l5 = UpSampling2D(size=(2, 2))(concatenate([enc_l1, dec_l4], axis=3))\n",
        "    # dec_l5 = UpSampling2D(size=(2, 2))(dec_l4)\n",
        "    # dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer10', padding='same')(dec_l5)\n",
        "    # # dec_l5 = BatchNormalization()(dec_l5)\n",
        "    # dec_l5 = Conv2D(num_filters, 3, activation='relu', name='decoder_layer9', padding='same')(dec_l5)\n",
        "    # dec_l5 = BatchNormalization()(dec_l5)\n",
        "    # dec_l5 = Dropout(0.4)(dec_l5)\n",
        "\n",
        "    # decoder_output = Conv2D(input_shape[2], 1, activation='relu', name='decoder_output', padding='same')(dec_l5)\n",
        "\n",
        "    # model = Model(inputs=[encoder_input], outputs=[class_output, decoder_output])  # class_output, decoder_output,\n",
        "    model = Model(inputs=[encoder_input], outputs=[class_output])  # class_output, decoder_output,\n",
        "\n",
        "    # Compiling model\n",
        "    # model.compile(optimizer=tf.keras.optimizers.Adam(),#learning_rate=0.0001, beta_1=0.9, beta_2=0.999,\n",
        "    #                                                   #epsilon=1e-07, amsgrad=False),\n",
        "    #               loss= {'class_output': 'categorical_crossentropy', 'decoder_output': 'mae'}, #loss_DSSIM},\n",
        "    #               loss_weights={'class_output': 0.01,  'decoder_output': 0.99},\n",
        "    #               metrics={'class_output': 'categorical_accuracy',  'decoder_output': 'mae'})#[DSSIM, DPSNR, 'mae']})\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                  loss= {'class_output': 'categorical_crossentropy'}, #loss_DSSIM},\n",
        "                  metrics={'class_output': 'categorical_accuracy'})#[DSSIM, DPSNR, 'mae']})    \n",
        "    model.summary()                        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8UEHjUVOo9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8758ce2d-875b-48b6-ac13-84d056a28a70"
      },
      "source": [
        "model = BUS_Final_Model()\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_input (InputLayer)   [(None, 120, 160, 120, 1) 0         \n",
            "_________________________________________________________________\n",
            "encoder_layer1 (Conv3D)      (None, 120, 160, 120, 8)  27008     \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 120, 160, 120, 8)  32        \n",
            "_________________________________________________________________\n",
            "encoder_layer2 (Conv3D)      (None, 120, 160, 120, 8)  1736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 120, 160, 120, 8)  32        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 120, 160, 120, 8)  0         \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 15, 20, 15, 8)     0         \n",
            "_________________________________________________________________\n",
            "encoder_output (Conv3D)      (None, 15, 20, 15, 8)     1736      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 36000)             0         \n",
            "_________________________________________________________________\n",
            "class_layer1 (Dense)         (None, 8)                 288008    \n",
            "_________________________________________________________________\n",
            "class_output (Dense)         (None, 2)                 18        \n",
            "=================================================================\n",
            "Total params: 318,570\n",
            "Trainable params: 318,538\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqgLnVODvL2Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "0fcb938f-3e87-4545-844d-590bdece6075"
      },
      "source": [
        "Input_train = np.squeeze(Input_train)\n",
        "Input_train = np.expand_dims(Input_train, axis=4)\n",
        "print(Input_train.shape)\n",
        "\n",
        "Input_val = np.expand_dims(Input_val, axis=4)\n",
        "print(Input_val.shape)\n",
        "\n",
        "Input_test = np.expand_dims(Input_test, axis=4)\n",
        "print(Input_test.shape)\n",
        "\n",
        "# out=model.predict(Input_train)\n",
        "# print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 120, 160, 120, 1)\n",
            "(4, 120, 160, 120, 1)\n",
            "(4, 120, 160, 120, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-90721a0623a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# out.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,8,120,160,120] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/encoder_layer2/Conv3D (defined at <ipython-input-11-a3afe7d62566>:11) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_predict_function_354]\n\nFunction call stack:\npredict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctiZpwdn4V4i"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSvqpQsOQyb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cb0488-5060-4f03-ae92-1d2e7afdd1df"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)\n",
        "checkpoint = ModelCheckpoint('models\\\\model-best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "# es = EarlyStopping(monitor='val_class_output_categorical_accuracy', mode='max', verbose=0, patience=10)\n",
        "\n",
        "# checkpoint = ModelCheckpoint('models\\\\model-best.h5', verbose=1,\n",
        "#                              monitor='val_class_output_categorical_accuracy',\n",
        "#                              save_best_only=True, mode='auto')\n",
        "  \n",
        "# history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train,'decoder_output': Input_train},             \n",
        "#                     validation_data=({'encoder_input': Input_val}, {'class_output': Label_val, 'decoder_output': Input_val}),                                      \n",
        "#                     batch_size=10, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n",
        "\n",
        "history = model.fit({'encoder_input': Input_train}, {'class_output': Label_train},\n",
        "                    validation_data=({'encoder_input': Input_val}, {'class_output': Label_val}),\n",
        "                    batch_size=1, epochs=100, shuffle=True, verbose=2, callbacks=[checkpoint, es])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "32/32 - 86s - loss: 7.4903 - categorical_accuracy: 0.7188 - val_loss: 0.7869 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.78693, saving model to models\\model-best.h5\n",
            "Epoch 2/100\n",
            "32/32 - 58s - loss: 0.8845 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.78693 to 0.69326, saving model to models\\model-best.h5\n",
            "Epoch 3/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69326 to 0.69325, saving model to models\\model-best.h5\n",
            "Epoch 4/100\n",
            "32/32 - 58s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.69325 to 0.69324, saving model to models\\model-best.h5\n",
            "Epoch 5/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.69324\n",
            "Epoch 6/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.69324\n",
            "Epoch 7/100\n",
            "32/32 - 57s - loss: 0.6937 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.69324\n",
            "Epoch 8/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.69324\n",
            "Epoch 9/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.69324 to 0.69323, saving model to models\\model-best.h5\n",
            "Epoch 10/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.69323\n",
            "Epoch 11/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.69323\n",
            "Epoch 12/100\n",
            "32/32 - 57s - loss: 0.6937 - categorical_accuracy: 0.5000 - val_loss: 0.6933 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.69323\n",
            "Epoch 13/100\n",
            "32/32 - 58s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.69323 to 0.69322, saving model to models\\model-best.h5\n",
            "Epoch 14/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69322 to 0.69320, saving model to models\\model-best.h5\n",
            "Epoch 15/100\n",
            "32/32 - 57s - loss: 0.6936 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.69320\n",
            "Epoch 16/100\n",
            "32/32 - 58s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.69320 to 0.69320, saving model to models\\model-best.h5\n",
            "Epoch 17/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69320 to 0.69319, saving model to models\\model-best.h5\n",
            "Epoch 18/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69319\n",
            "Epoch 19/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69319\n",
            "Epoch 20/100\n",
            "32/32 - 57s - loss: 0.6937 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69319\n",
            "Epoch 21/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69319 to 0.69319, saving model to models\\model-best.h5\n",
            "Epoch 22/100\n",
            "32/32 - 57s - loss: 0.6938 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.69319 to 0.69316, saving model to models\\model-best.h5\n",
            "Epoch 23/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69316\n",
            "Epoch 24/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69316\n",
            "Epoch 25/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69316\n",
            "Epoch 26/100\n",
            "32/32 - 57s - loss: 0.6938 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69316\n",
            "Epoch 27/100\n",
            "32/32 - 57s - loss: 0.6936 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.69316 to 0.69316, saving model to models\\model-best.h5\n",
            "Epoch 28/100\n",
            "32/32 - 58s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69316\n",
            "Epoch 29/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69316\n",
            "Epoch 30/100\n",
            "32/32 - 57s - loss: 0.6932 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69316\n",
            "Epoch 31/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69316\n",
            "Epoch 32/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69316\n",
            "Epoch 33/100\n",
            "32/32 - 57s - loss: 0.6936 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69316\n",
            "Epoch 34/100\n",
            "32/32 - 58s - loss: 0.6936 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69316\n",
            "Epoch 35/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69316\n",
            "Epoch 36/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.69316 to 0.69316, saving model to models\\model-best.h5\n",
            "Epoch 37/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69316\n",
            "Epoch 38/100\n",
            "32/32 - 58s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69316\n",
            "Epoch 39/100\n",
            "32/32 - 57s - loss: 0.6939 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69316\n",
            "Epoch 40/100\n",
            "32/32 - 57s - loss: 0.6932 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69316\n",
            "Epoch 41/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69316\n",
            "Epoch 42/100\n",
            "32/32 - 57s - loss: 0.6935 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.69316 to 0.69316, saving model to models\\model-best.h5\n",
            "Epoch 43/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.69316 to 0.69316, saving model to models\\model-best.h5\n",
            "Epoch 44/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.69316 to 0.69315, saving model to models\\model-best.h5\n",
            "Epoch 45/100\n",
            "32/32 - 57s - loss: 0.6932 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69315\n",
            "Epoch 46/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.69315 to 0.69315, saving model to models\\model-best.h5\n",
            "Epoch 47/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69315\n",
            "Epoch 48/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.69315 to 0.69315, saving model to models\\model-best.h5\n",
            "Epoch 49/100\n",
            "32/32 - 57s - loss: 0.6932 - categorical_accuracy: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69315\n",
            "Epoch 50/100\n",
            "32/32 - 57s - loss: 0.6933 - categorical_accuracy: 0.5000 - val_loss: 0.6931 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69315\n",
            "Epoch 51/100\n",
            "32/32 - 57s - loss: 0.6934 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69315\n",
            "Epoch 52/100\n",
            "32/32 - 57s - loss: 0.6932 - categorical_accuracy: 0.5000 - val_loss: 0.6932 - val_categorical_accuracy: 0.5000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69315\n",
            "Epoch 53/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VfQ8rze4ltS"
      },
      "source": [
        "# Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgx7ywtgQdR4"
      },
      "source": [
        "del model\n",
        "model = load_model('models\\model-best.h5')              \n",
        "# [Label_train_pred, decoder_output_train_pred] = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "# [Label_val_pred, decoder_output_val_pred] = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "# [Label_test_pred, decoder_output_test_pred] = model.predict(Input_test, batch_size=1, verbose=0)\n",
        "Label_train_pred = model.predict(Input_train, batch_size=1, verbose=0)\n",
        "Label_val_pred = model.predict(Input_val, batch_size=1, verbose=0)\n",
        "Label_test_pred = model.predict(Input_test, batch_size=1, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6pOqLuSoOf0"
      },
      "source": [
        "Training_Stats = []\n",
        "# train Measures\n",
        "train_acc = accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "train_spe = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Sensitivity']\n",
        "train_sen = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['Specificity']\n",
        "train_f1 = pmeasure(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))['F1-Score']\n",
        "\n",
        "train_bacc = balanced_accuracy_score(Label_train.argmax(axis=1), Label_train_pred.argmax(axis=1))\n",
        "\n",
        "Training_Stats.append([train_acc,\n",
        "                       train_sen, \n",
        "                       train_spe, \n",
        "                       train_f1, \n",
        "                       train_bacc])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nMkb1OAAjqa"
      },
      "source": [
        "Validation_Stats = []\n",
        "# val Measures\n",
        "val_acc = accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "val_spe = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Sensitivity']\n",
        "val_sen = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['Specificity']\n",
        "val_f1 = pmeasure(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))['F1-Score']\n",
        "\n",
        "val_bacc = balanced_accuracy_score(Label_val.argmax(axis=1), Label_val_pred.argmax(axis=1))\n",
        "\n",
        "Validation_Stats.append([val_acc,\n",
        "                       val_sen, \n",
        "                       val_spe, \n",
        "                       val_f1, \n",
        "                       val_bacc])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrRMPjLIA96T"
      },
      "source": [
        "Testing_Stats = []\n",
        "# Test Measures\n",
        "tst_acc = accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "tst_spe = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Sensitivity']\n",
        "tst_sen = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['Specificity']\n",
        "tst_f1 = pmeasure(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))['F1-Score']\n",
        "\n",
        "tst_bacc = balanced_accuracy_score(Label_test.argmax(axis=1), Label_test_pred.argmax(axis=1))\n",
        "\n",
        "Testing_Stats.append([tst_acc,\n",
        "                       tst_sen, \n",
        "                       tst_spe, \n",
        "                       tst_f1, \n",
        "                       tst_bacc])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPTontB8qOt_"
      },
      "source": [
        "Train_Statistics = np.asarray(Training_Stats)\n",
        "Val_Statistics = np.asarray(Validation_Stats)\n",
        "Test_Statistics = np.asarray(Testing_Stats)\n",
        "\n",
        "# Show Classification/Reconstruction Statistics\n",
        "Show_Statistics('Training Results', Train_Statistics[0])\n",
        "Show_Statistics('Validation Results', Val_Statistics[0])\n",
        "Show_Statistics('Test Results', Test_Statistics[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}